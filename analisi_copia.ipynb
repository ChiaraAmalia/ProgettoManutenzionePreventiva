{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTAZIONE LIBRERIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import array \n",
    "import scipy.signal as signal\n",
    "#Processing libraries\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Model libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Testing libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMBIO PATH PER ANALISI RISPETTO AL LOG DI VOLO CONSIDERAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = os.path.abspath(os.getcwd())\n",
    "print(path_file)\n",
    "os.chdir(path_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARRAY CHE CONTERRA' I DATAFRAME DI CIASCUN VOLO (PER UN TOTALE DI 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(path_file,'*.csv') #lista di tutti gli elementi di estensione csv nella cartella path\n",
    "csv_list = glob.glob(data_path) #converte data path in un output Unix-like (ls) (*.csv -> lista di elementi con estensione csv)\n",
    "dataframe_collection = []\n",
    "df_1 = pd.DataFrame()\n",
    "\n",
    "# ciclo per scorrere tutti i csv\n",
    "for csv_file in csv_list: # ciclo che scorre i csv nella cartella path\n",
    "    df = pd.read_csv(csv_file) \n",
    "    #df = df.to_numpy()\n",
    "    #df = df.to_records(index=False)\n",
    "    #dataframe_collection.append(df)\n",
    "    df_1 = df_1.append(df,ignore_index=True)\n",
    "    #print(df)\n",
    "\n",
    "# ciclo per scorrere tutti i csv\n",
    "for csv_file in csv_list: # ciclo che scorre i csv nella cartella path\n",
    "    df = pd.read_csv(csv_file) \n",
    "    #df = df.to_numpy()\n",
    "    df = df.to_records(index=False)\n",
    "    dataframe_collection.append(df)\n",
    "\n",
    "#prova = pd.DataFrame(dataframe_collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.sample(frac=1).reset_index(drop=True)\n",
    "print(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.drop(['tempo'], axis=1)\n",
    "print(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guasto = df_1[\"Guasto\"]\n",
    "\n",
    "df_1 = df_1.drop([\"Guasto\"], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_1 = pd.DataFrame(scaler.fit_transform(df_1.values), columns=df_1.columns, index=df_1.index)\n",
    "df_1[\"Guasto\"] = guasto\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = set()\n",
    "correlation = df_1[df_1.columns[1:]].corr()['Guasto'][:].abs()\n",
    "correlation = correlation.loc[lambda x : ((x > 0.7) & (x < 1))]\n",
    "index = correlation.index\n",
    "print(index)\n",
    "df_1 = df_1.drop([col for col in df_1.columns if col in index],axis=1)\n",
    "print(df_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''correlated_features = set()\n",
    "correlation_matrix = df_1.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "print(len(correlated_features))\n",
    "print(correlated_features)\n",
    "\n",
    "df_1 = df_1.drop([col for col in df_1.columns if col in correlated_features],axis=1)\n",
    "print(df_1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [470]\n",
    "df_test = df_1.query('index in @index_list')\n",
    "#df_test = pd.DataFrame(df_test)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1 = df_1.drop([df_1.index[1330], df_1.index[1331]])\n",
    "df_1 = df_1.drop([df_1.index[470]])\n",
    "print(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guasto_10 = df_1[df_1['Guasto'] == 2]\n",
    "no_guasto = df_1[df_1['Guasto'] == 0]\n",
    "guasto_5 = df_1[df_1['Guasto'] == 1]\n",
    "\n",
    "print(\"No guasto: \", len(no_guasto), \"Guasto 5%: \", len(guasto_5), \"Guasto 10%: \", len(guasto_10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilanciamento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bilanciamento del dataset con undersampling rispetto al no_guasto\n",
    "balanced_d = pd.concat([no_guasto, guasto_10.sample(len(no_guasto)), guasto_5.sample(len(no_guasto))])\n",
    "x = balanced_d.iloc[:,:-1]\n",
    "y = balanced_d.iloc[:,-1:]\n",
    "balanced_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_1.iloc[:,:-1]\n",
    "y = df_1.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "x, y = oversample.fit_resample(x,y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif, f_regression\n",
    "fvalue_Best = SelectKBest(score_func=f_regression, k=15)\n",
    "fit = fvalue_Best.fit(x, y)\n",
    "print(fit)\n",
    "print(fvalue_Best.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_score = pd.DataFrame(fit.scores_)\n",
    "features_score = zscore(features_score)\n",
    "features = pd.DataFrame(x.columns)\n",
    "feature_score = pd.concat([features,features_score],axis=1)\n",
    "# Assigning column names\n",
    "feature_score.columns = [\"Input_Features\",\"F_Score\"]\n",
    "print(feature_score.nlargest(15,columns=\"F_Score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "index_false=[]\n",
    "for el in list(fvalue_Best.get_support()):\n",
    "    if not el:\n",
    "        index_false.append(i)\n",
    "    i=i+1\n",
    "print(index_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.drop(x.columns[index_false],axis = 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "ax = sns.heatmap(x[x.columns].corr(),cmap=\"Blues\",annot=True,annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = set()\n",
    "correlation_matrix = x.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if (correlation_matrix.iloc[i, j] > 0.8) | (correlation_matrix.iloc[i, j] < -0.5):\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "x = x.drop([col for col in x.columns if col in correlated_features],axis=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "ax = sns.heatmap(x[x.columns].corr(),cmap=\"Blues\",annot=True,annot_kws={\"size\": 18})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separazione del dataset in training e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random state\n",
    "rs = 42\n",
    "\n",
    "# Split the data to check which algorithms learn better (later on we can check )\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=rs)\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# look at the shape of the data (many problems can arise from wrong shape)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "#sel_mutual = SelectKBest(mutual_info_classif, k=6)\n",
    "#X_train_mutual = sel_mutual.fit_transform(x_train, y_train)\n",
    "#print(sel_mutual.get_support())\n",
    "\n",
    "#i=0\n",
    "#index_false=[]\n",
    "#for el in list(sel_mutual.get_support()):\n",
    "#    if not el:\n",
    "#        index_false.append(i)\n",
    "#    i=i+1\n",
    "#print(index_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train=x_train.drop(x_train.columns[index_false],axis = 1)\n",
    "#x_test=x_test.drop(x_test.columns[index_false],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of classifiers:\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state = rs),\n",
    "    DecisionTreeClassifier(random_state=rs),\n",
    "    RandomForestClassifier(n_estimators = 10, random_state=rs),\n",
    "    GradientBoostingClassifier(random_state= rs, n_estimators=10),\n",
    "    AdaBoostClassifier(n_estimators=10, random_state= rs),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    SVC(probability=True),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(n_neighbors=10)\n",
    "]\n",
    "\n",
    "# List of results that will occure:\n",
    "clf_name = [] # names of the classifiers\n",
    "model_results = pd.DataFrame.copy(y_test) #resulting of prediction from the models\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5) #cross-validation\n",
    "cv_results = [] # scores from cross validation\n",
    "cv_acc = [] # mean accuracy from cross validation, need to maximize\n",
    "cv_std = [] # standard deviation from cross validation, need to minimise\n",
    "\n",
    "cnfm = [] #confusion matrix\n",
    "clr = [] #classification report\n",
    "roc_auc = [] #roc curve:\n",
    "roc_tpr = []\n",
    "roc_fpr = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the algorithms and results\n",
    "for clf in classifiers:\n",
    "    name = clf.__class__.__name__\n",
    "    clf_name.append(name)\n",
    "    \n",
    "    #fitting and predictions\n",
    "    model = clf.fit(x_train, y_train.values.ravel())\n",
    "    y_pred = model.predict(x_test)\n",
    "    model_results[name] = y_pred\n",
    "    \n",
    "    #accuracy and log loss\n",
    "    cv_results.append(cross_val_score(clf, x_train, y_train.values.ravel(), scoring = \"accuracy\",cv = kfold))\n",
    "    acc = round(accuracy_score(y_test.values.ravel(), y_pred), 2) #need to maximize\n",
    "    train_pred = clf.predict_proba(x_test)\n",
    "    print(f'Accuracy: {acc} \\t ---> {name} ')\n",
    "    \n",
    "    #confusion matrix, clasification report, roc curve\n",
    "    cnfm.append(confusion_matrix(y_test.values.ravel(), y_pred))\n",
    "    clr.append(classification_report(y_test.values.ravel(), y_pred))\n",
    "    fpr, tpr, thresholds = roc_curve(y_pred, y_test.values.ravel(), pos_label=1)\n",
    "    roc_auc.append(auc(fpr, tpr))\n",
    "    roc_tpr.append(tpr)\n",
    "    roc_fpr.append(fpr)\n",
    "    \n",
    "\n",
    "for i in cv_results:\n",
    "    cv_acc.append(i.mean())\n",
    "    cv_std.append(i.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrici di confusione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Confusion matrixes (not-normalized confusion matrix)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=1.4)\n",
    "for i in range(len(classifiers)):\n",
    "    plt.subplot(3,3,i+1) #adjust this acourding to the number of algorithms\n",
    "    sns.heatmap(cnfm[i], annot=True, fmt=\"d\",cmap=\"Reds\")\n",
    "    plt.subplots_adjust(hspace = 0.5)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(clf_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification reports\n",
    "for i in range(len(classifiers)):\n",
    "    print (f\"{clf_name[i]} Classification Report:\" )\n",
    "    print (clr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = SVC(probability=True).fit(x_train, y_train)\n",
    "clf = GaussianNB().fit(x_train, y_train)\n",
    "scores = cross_val_score(clf, x_test, y_test, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING DEL MODELLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop values to avoid multicolinearity\n",
    "df_test = df_test.drop(['Guasto'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.drop(df_test.columns[index_false],axis = 1)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guasto_pred = pd.Series(clf.predict(df_test), name='Guasto_test')\n",
    "guasto_pred.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
